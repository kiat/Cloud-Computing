# How to Compile and Run ####

Prerequisite:

* Apache Maven
* Java JDK 1.6 or higher

The java main class is:

edu.cs.utexas.HadoopEx.EMDriver 

Input file:  points.txt

Specify your own Output directory like "output"


## Create a JAR Using Maven 

To compile the project and create a single jar file with all dependencies: 
	
	```mvn clean package```


## Running:

```EMDriver input output```


```java -jar target/GMM-example-0.1-SNAPSHOT-jar-with-dependencies.jar points.txt output ```


# Gaussian Mixture Model (GMM)

Let the dataset be:


$X = \{x_1, x_2, \dots, x_N\},
\quad x_i \in \mathbb{R}^d
$
Assume the number of mixture components is $K$.



## Mixture Model Definition

The probability density function of a Gaussian Mixture Model is:

```math
p(x_i \mid \Theta)
=
\sum_{k=1}^{K}
\pi_k \, \mathcal{N}(x_i \mid \mu_k, \Sigma_k)
```

where:

* $\pi_k$ are the mixing coefficients,
* $\mu_k \in \mathbb{R}^d$ are the mean vectors,
* $\Sigma_k \in \mathbb{R}^{d \times d}$ are the covariance matrices,
* $\sum_{k=1}^{K} \pi_k = 1$,
* $\pi_k \ge 0$.


# Multivariate Gaussian Density


Latent variables:

```math
\Huge
z_{ik} =
\begin{cases}
1 & \text{if } x_i \text{ is generated by component } k \\
0 & \text{otherwise}
\end{cases}
```

## Complete-Data Log-Likelihood


```math
\log p(X,Z \mid \Theta)
=
\sum_{i=1}^{N}
\sum_{k=1}^{K}
z_{ik}
\left(
\log \pi_k
+
\log \mathcal{N}(x_i \mid \mu_k, \Sigma_k)
\right)
```

## E-Step

Compute the posterior responsibilities:

```math
\Huge
\gamma_{ik}
=
P(z_{ik} = 1 \mid x_i)
=
\frac{
\pi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k)
}{
\sum_{j=1}^{K}
\pi_j \mathcal{N}(x_i \mid \mu_j, \Sigma_j)
}
```


## M-Step

```math
\Huge
N_k
=
\sum_{i=1}^{N}
\gamma_{ik}
```


## Update Mixing Coefficients

```math
\Huge
\pi_k^{new}
=
\frac{N_k}{N}
```

## Update Means

```math
\Huge
\mu_k^{new}
=
\frac{1}{N_k}
\sum_{i=1}^{N}
\gamma_{ik} x_i
```


# Update Covariance Matrices

```math
\Large
\Sigma_k^{new}
=
\frac{1}{N_k}
\sum_{i=1}^{N}
\gamma_{ik}
(x_i - \mu_k^{new})
(x_i - \mu_k^{new})^T
```


# Observed Log-Likelihood


```math
\Large
\log p(X \mid \Theta)
=
\sum_{i=1}^{N}
\log
\left(
\sum_{k=1}^{K}
\pi_k
\mathcal{N}(x_i \mid \mu_k, \Sigma_k)
\right)
```


# Log-Sum-Exp Trick (Numerical Stability)

```math
\Large
\log \sum_{k} e^{a_k}
=
m +
\log
\sum_{k}
e^{a_k - m}
\quad
\text{where}
\quad
m = \max_k a_k
```











